{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in csv\n",
    "posts = pd.read_csv('./datasets/ethbtc_posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TPDNM</td>\n",
       "      <td>Is it possible for an invalid tx to stay in th...</td>\n",
       "      <td>ethereum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nhttps://www.coindesk.com/16-ethereum-predict...</td>\n",
       "      <td>16 Ethereum Predictions From a Crypto Oracle</td>\n",
       "      <td>ethereum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TPDNM</td>\n",
       "      <td>What do you think is the max value of Eth befo...</td>\n",
       "      <td>ethereum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TPDNM</td>\n",
       "      <td>MYID Coins are currently an ethereum erc20</td>\n",
       "      <td>ethereum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TPDNM</td>\n",
       "      <td>90% of Ethereum Addresses are In Profit Follow...</td>\n",
       "      <td>ethereum</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            selftext  \\\n",
       "0                                              TPDNM   \n",
       "1  \\nhttps://www.coindesk.com/16-ethereum-predict...   \n",
       "2                                              TPDNM   \n",
       "3                                              TPDNM   \n",
       "4                                              TPDNM   \n",
       "\n",
       "                                               title subreddit  \n",
       "0  Is it possible for an invalid tx to stay in th...  ethereum  \n",
       "1       16 Ethereum Predictions From a Crypto Oracle  ethereum  \n",
       "2  What do you think is the max value of Eth befo...  ethereum  \n",
       "3         MYID Coins are currently an ethereum erc20  ethereum  \n",
       "4  90% of Ethereum Addresses are In Profit Follow...  ethereum  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "selftext     0\n",
       "title        0\n",
       "subreddit    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for null values \n",
    "posts.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of TPDNM. \n",
    "# After doing a couple rounds of EDA and modelling, \n",
    "# I needed to reduce the amount of data\n",
    "# for memory purposes\n",
    "# and decided that removing the TPDNM  suffices\n",
    "posts = posts[posts.selftext != 'TPDNM']\n",
    "posts.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#posts.set_index('index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\\nhttps://www.coindesk.com/16-ethereum-predict...</td>\n",
       "      <td>16 Ethereum Predictions From a Crypto Oracle</td>\n",
       "      <td>ethereum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>Alright so I’ve been on the ETH train for some...</td>\n",
       "      <td>Upside of Ether?</td>\n",
       "      <td>ethereum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>i tried almost a year and a half ago to get in...</td>\n",
       "      <td>i need help</td>\n",
       "      <td>ethereum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>Hi guys, I am not very familiar with ethereum ...</td>\n",
       "      <td>New to this... is now a good time to invest in...</td>\n",
       "      <td>ethereum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>Hi everyone, I think this is the most suited s...</td>\n",
       "      <td>Should I stake?</td>\n",
       "      <td>ethereum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23</td>\n",
       "      <td>Spoiler alert.. a lot.  \\nWe go frontrun on tr...</td>\n",
       "      <td>We took a tour around the dark forest to see h...</td>\n",
       "      <td>ethereum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>24</td>\n",
       "      <td>So The Secret Network Ethereum bridge is live....</td>\n",
       "      <td>More secretEther users needed! Privacy fans un...</td>\n",
       "      <td>ethereum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>29</td>\n",
       "      <td>Hi ethereum holders/investors/stakers etc.\\n\\n...</td>\n",
       "      <td>Max supply of ETH</td>\n",
       "      <td>ethereum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>33</td>\n",
       "      <td>\\nIf you’ve been keeping up with Ripple then y...</td>\n",
       "      <td>XRP right now lol</td>\n",
       "      <td>ethereum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>35</td>\n",
       "      <td>I know the basics of crypto but I’m not the mo...</td>\n",
       "      <td>Is this a bubble?</td>\n",
       "      <td>ethereum</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           selftext  \\\n",
       "0      1  \\nhttps://www.coindesk.com/16-ethereum-predict...   \n",
       "1      8  Alright so I’ve been on the ETH train for some...   \n",
       "2     15  i tried almost a year and a half ago to get in...   \n",
       "3     16  Hi guys, I am not very familiar with ethereum ...   \n",
       "4     21  Hi everyone, I think this is the most suited s...   \n",
       "5     23  Spoiler alert.. a lot.  \\nWe go frontrun on tr...   \n",
       "6     24  So The Secret Network Ethereum bridge is live....   \n",
       "7     29  Hi ethereum holders/investors/stakers etc.\\n\\n...   \n",
       "8     33  \\nIf you’ve been keeping up with Ripple then y...   \n",
       "9     35  I know the basics of crypto but I’m not the mo...   \n",
       "\n",
       "                                               title subreddit  \n",
       "0       16 Ethereum Predictions From a Crypto Oracle  ethereum  \n",
       "1                                   Upside of Ether?  ethereum  \n",
       "2                                        i need help  ethereum  \n",
       "3  New to this... is now a good time to invest in...  ethereum  \n",
       "4                                    Should I stake?  ethereum  \n",
       "5  We took a tour around the dark forest to see h...  ethereum  \n",
       "6  More secretEther users needed! Privacy fans un...  ethereum  \n",
       "7                                  Max supply of ETH  ethereum  \n",
       "8                                  XRP right now lol  ethereum  \n",
       "9                                  Is this a bubble?  ethereum  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - It's unclear to me if combining the 'selftext' and 'title' features into one 'text' column will introduce bias into my model because in my opinion language in titles and texts can be interpreted differently. For instance, titles tend to be more succinct and only use key terms whereas texts tend to have fleshed out ideas. In other words, it might be worth it to treat them as separate variables. For now, since I'm not familiar with Reddit ettiquete, I will combine them, and if my model doesn't perform wel, I will consider splitting them into their own features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-70-90ca28aadefc>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  posts['text'] = posts['selftext'] + posts['title']\n"
     ]
    }
   ],
   "source": [
    "posts['text'] = posts['selftext'] + posts['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nhttps://www.coindesk.com/16-ethereum-predict...</td>\n",
       "      <td>ethereum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alright so I’ve been on the ETH train for some...</td>\n",
       "      <td>ethereum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i tried almost a year and a half ago to get in...</td>\n",
       "      <td>ethereum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi guys, I am not very familiar with ethereum ...</td>\n",
       "      <td>ethereum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi everyone, I think this is the most suited s...</td>\n",
       "      <td>ethereum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Spoiler alert.. a lot.  \\nWe go frontrun on tr...</td>\n",
       "      <td>ethereum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>So The Secret Network Ethereum bridge is live....</td>\n",
       "      <td>ethereum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hi ethereum holders/investors/stakers etc.\\n\\n...</td>\n",
       "      <td>ethereum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\nIf you’ve been keeping up with Ripple then y...</td>\n",
       "      <td>ethereum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I know the basics of crypto but I’m not the mo...</td>\n",
       "      <td>ethereum</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text subreddit\n",
       "0  \\nhttps://www.coindesk.com/16-ethereum-predict...  ethereum\n",
       "1  Alright so I’ve been on the ETH train for some...  ethereum\n",
       "2  i tried almost a year and a half ago to get in...  ethereum\n",
       "3  Hi guys, I am not very familiar with ethereum ...  ethereum\n",
       "4  Hi everyone, I think this is the most suited s...  ethereum\n",
       "5  Spoiler alert.. a lot.  \\nWe go frontrun on tr...  ethereum\n",
       "6  So The Secret Network Ethereum bridge is live....  ethereum\n",
       "7  Hi ethereum holders/investors/stakers etc.\\n\\n...  ethereum\n",
       "8  \\nIf you’ve been keeping up with Ripple then y...  ethereum\n",
       "9  I know the basics of crypto but I’m not the mo...  ethereum"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts[['text', 'subreddit']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ethereum    5780\n",
       "Bitcoin     4852\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just want to check value counts \n",
    "posts['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scraping data for Ethereum was a pain. For some reason, there were a ton of Ethereum posts that were either deleted, removed, or empty. I had to run more loops and ended up pulling more data. Luckily the ratio above seems like a pretty even distribution of both posts for modeling purposes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I want to turn the 'subreddit' variable into a dummy variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ethereum', 'Bitcoin'], dtype=object)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts['subreddit'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-74-66468166377f>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  posts['subreddit'] = posts['subreddit'].map({'Bitcoin': 0, 'ethereum':1})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\\nhttps://www.coindesk.com/16-ethereum-predict...</td>\n",
       "      <td>16 Ethereum Predictions From a Crypto Oracle</td>\n",
       "      <td>1</td>\n",
       "      <td>\\nhttps://www.coindesk.com/16-ethereum-predict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>Alright so I’ve been on the ETH train for some...</td>\n",
       "      <td>Upside of Ether?</td>\n",
       "      <td>1</td>\n",
       "      <td>Alright so I’ve been on the ETH train for some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>i tried almost a year and a half ago to get in...</td>\n",
       "      <td>i need help</td>\n",
       "      <td>1</td>\n",
       "      <td>i tried almost a year and a half ago to get in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>Hi guys, I am not very familiar with ethereum ...</td>\n",
       "      <td>New to this... is now a good time to invest in...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hi guys, I am not very familiar with ethereum ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>Hi everyone, I think this is the most suited s...</td>\n",
       "      <td>Should I stake?</td>\n",
       "      <td>1</td>\n",
       "      <td>Hi everyone, I think this is the most suited s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           selftext  \\\n",
       "0      1  \\nhttps://www.coindesk.com/16-ethereum-predict...   \n",
       "1      8  Alright so I’ve been on the ETH train for some...   \n",
       "2     15  i tried almost a year and a half ago to get in...   \n",
       "3     16  Hi guys, I am not very familiar with ethereum ...   \n",
       "4     21  Hi everyone, I think this is the most suited s...   \n",
       "\n",
       "                                               title  subreddit  \\\n",
       "0       16 Ethereum Predictions From a Crypto Oracle          1   \n",
       "1                                   Upside of Ether?          1   \n",
       "2                                        i need help          1   \n",
       "3  New to this... is now a good time to invest in...          1   \n",
       "4                                    Should I stake?          1   \n",
       "\n",
       "                                                text  \n",
       "0  \\nhttps://www.coindesk.com/16-ethereum-predict...  \n",
       "1  Alright so I’ve been on the ETH train for some...  \n",
       "2  i tried almost a year and a half ago to get in...  \n",
       "3  Hi guys, I am not very familiar with ethereum ...  \n",
       "4  Hi everyone, I think this is the most suited s...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts['subreddit'] = posts['subreddit'].map({'Bitcoin': 0, 'ethereum':1})\n",
    "posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is a lot of cleaning to do. \n",
    "- First let's take out the special characters and symbols. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-75-5473c8e30834>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  posts['text'] = posts['text'].str.replace('[^\\w\\s]', '') # for punctuation\n"
     ]
    }
   ],
   "source": [
    "# This code adapted from this https://stackoverflow.com/questions/47423854/sklearn-adding-lemmatizer-to-countvectorizer\n",
    "# the purpose of it is to remove digits, words, and punctuation \n",
    "# in order to lemmatize my text variable within CountVectorizer \n",
    "\n",
    "#posts['text'] = posts['text'].str.replace('\\d+', '') # for digits\n",
    "#posts['text'] = posts['text'].str.replace(r'(\\b\\w{1,2}\\b)', '') # for words\n",
    "posts['text'] = posts['text'].str.replace('[^\\w\\s]', '') # for punctuation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   The crypto-community is an extremely vibrant and typically young group of people. One of its characteristics is embracing the speculative \"wild west\" nature of the space which sometimes involves using sarcastic, dramatic, and even spiteful language and therefore a bunch of mispelled words, made-up words, and oftentimes ALL CAPS. \n",
    "   \n",
    "   An area of interest that I would like to explore during this project is a sentiment analysis of subreddits within the Bitcoin and Ethereum communities. Each community has distinct characteristics, akin to different nation-states. Some distinctions are rooted in possessing different schools of ecomomic thought. For example, Bitcoin maxis would attest that Bitcoin is a more pure form of Austrian economics than Ethereum, which makes Bitcoin a more superior decentralized medium of exchange. In this regard, I think it would be interesting to investigate the community's differences using subreddit posts.\n",
    "   \n",
    "   Even further, there are several communities within the Ethereum community. A simple positive/negative sentiment analysis on subreddit posts of these communities with respect to the overall Bitcoin and Ethereum communities could shed light on trading and/or investing advantages.```\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Even though interpreting a characters case might shed light on the sentiment of the text, for this model's purposes, I want to lower all the characters cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-76-6e19ad461dcf>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  posts['text'] = [i.lower() for i in posts['text']]\n"
     ]
    }
   ],
   "source": [
    "# lowercase all text\n",
    "posts['text'] = [i.lower() for i in posts['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I also want a status length column that counts all of the texts characters (including white spaces). I will be able to observe texts that are outliers ie texts that might reuse the same words an unusual amount of time that could potentially skew my model's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-77-f83c8d6a8312>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  posts['status_length']  =  [len(posts['text'][i]) for i in range(len(posts['text']))]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "      <th>status_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\\nhttps://www.coindesk.com/16-ethereum-predict...</td>\n",
       "      <td>16 Ethereum Predictions From a Crypto Oracle</td>\n",
       "      <td>1</td>\n",
       "      <td>\\nhttpswwwcoindeskcom16ethereumpredictionscryp...</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>Alright so I’ve been on the ETH train for some...</td>\n",
       "      <td>Upside of Ether?</td>\n",
       "      <td>1</td>\n",
       "      <td>alright so ive been on the eth train for some ...</td>\n",
       "      <td>705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>i tried almost a year and a half ago to get in...</td>\n",
       "      <td>i need help</td>\n",
       "      <td>1</td>\n",
       "      <td>i tried almost a year and a half ago to get in...</td>\n",
       "      <td>816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>Hi guys, I am not very familiar with ethereum ...</td>\n",
       "      <td>New to this... is now a good time to invest in...</td>\n",
       "      <td>1</td>\n",
       "      <td>hi guys i am not very familiar with ethereum b...</td>\n",
       "      <td>390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>Hi everyone, I think this is the most suited s...</td>\n",
       "      <td>Should I stake?</td>\n",
       "      <td>1</td>\n",
       "      <td>hi everyone i think this is the most suited su...</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           selftext  \\\n",
       "0      1  \\nhttps://www.coindesk.com/16-ethereum-predict...   \n",
       "1      8  Alright so I’ve been on the ETH train for some...   \n",
       "2     15  i tried almost a year and a half ago to get in...   \n",
       "3     16  Hi guys, I am not very familiar with ethereum ...   \n",
       "4     21  Hi everyone, I think this is the most suited s...   \n",
       "\n",
       "                                               title  subreddit  \\\n",
       "0       16 Ethereum Predictions From a Crypto Oracle          1   \n",
       "1                                   Upside of Ether?          1   \n",
       "2                                        i need help          1   \n",
       "3  New to this... is now a good time to invest in...          1   \n",
       "4                                    Should I stake?          1   \n",
       "\n",
       "                                                text  status_length  \n",
       "0  \\nhttpswwwcoindeskcom16ethereumpredictionscryp...             97  \n",
       "1  alright so ive been on the eth train for some ...            705  \n",
       "2  i tried almost a year and a half ago to get in...            816  \n",
       "3  hi guys i am not very familiar with ethereum b...            390  \n",
       "4  hi everyone i think this is the most suited su...            662  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create column with status lengths\n",
    "posts['status_length']  =  [len(posts['text'][i]) for i in range(len(posts['text']))]\n",
    "posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Same goes for word counts! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-79-ff003d58b6e9>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  posts['word_count'] = [len(posts['text'][i].split()) for i in range(len(posts['text']))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      7\n",
       "1    142\n",
       "2    156\n",
       "3     76\n",
       "4    129\n",
       "Name: word_count, dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create column with word counts\n",
    "posts['word_count'] = [len(posts['text'][i].split()) for i in range(len(posts['text']))]\n",
    "posts['word_count'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "      <th>status_length</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\\nhttps://www.coindesk.com/16-ethereum-predict...</td>\n",
       "      <td>16 Ethereum Predictions From a Crypto Oracle</td>\n",
       "      <td>1</td>\n",
       "      <td>\\nhttpswwwcoindeskcom16ethereumpredictionscryp...</td>\n",
       "      <td>97</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>Alright so I’ve been on the ETH train for some...</td>\n",
       "      <td>Upside of Ether?</td>\n",
       "      <td>1</td>\n",
       "      <td>alright so ive been on the eth train for some ...</td>\n",
       "      <td>705</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>i tried almost a year and a half ago to get in...</td>\n",
       "      <td>i need help</td>\n",
       "      <td>1</td>\n",
       "      <td>i tried almost a year and a half ago to get in...</td>\n",
       "      <td>816</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>Hi guys, I am not very familiar with ethereum ...</td>\n",
       "      <td>New to this... is now a good time to invest in...</td>\n",
       "      <td>1</td>\n",
       "      <td>hi guys i am not very familiar with ethereum b...</td>\n",
       "      <td>390</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>Hi everyone, I think this is the most suited s...</td>\n",
       "      <td>Should I stake?</td>\n",
       "      <td>1</td>\n",
       "      <td>hi everyone i think this is the most suited su...</td>\n",
       "      <td>662</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23</td>\n",
       "      <td>Spoiler alert.. a lot.  \\nWe go frontrun on tr...</td>\n",
       "      <td>We took a tour around the dark forest to see h...</td>\n",
       "      <td>1</td>\n",
       "      <td>spoiler alert a lot  \\nwe go frontrun on trans...</td>\n",
       "      <td>396</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>24</td>\n",
       "      <td>So The Secret Network Ethereum bridge is live....</td>\n",
       "      <td>More secretEther users needed! Privacy fans un...</td>\n",
       "      <td>1</td>\n",
       "      <td>so the secret network ethereum bridge is live ...</td>\n",
       "      <td>874</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>29</td>\n",
       "      <td>Hi ethereum holders/investors/stakers etc.\\n\\n...</td>\n",
       "      <td>Max supply of ETH</td>\n",
       "      <td>1</td>\n",
       "      <td>hi ethereum holdersinvestorsstakers etc\\n\\nis ...</td>\n",
       "      <td>142</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>33</td>\n",
       "      <td>\\nIf you’ve been keeping up with Ripple then y...</td>\n",
       "      <td>XRP right now lol</td>\n",
       "      <td>1</td>\n",
       "      <td>\\nif youve been keeping up with ripple then yo...</td>\n",
       "      <td>272</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>35</td>\n",
       "      <td>I know the basics of crypto but I’m not the mo...</td>\n",
       "      <td>Is this a bubble?</td>\n",
       "      <td>1</td>\n",
       "      <td>i know the basics of crypto but im not the mos...</td>\n",
       "      <td>539</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           selftext  \\\n",
       "0      1  \\nhttps://www.coindesk.com/16-ethereum-predict...   \n",
       "1      8  Alright so I’ve been on the ETH train for some...   \n",
       "2     15  i tried almost a year and a half ago to get in...   \n",
       "3     16  Hi guys, I am not very familiar with ethereum ...   \n",
       "4     21  Hi everyone, I think this is the most suited s...   \n",
       "5     23  Spoiler alert.. a lot.  \\nWe go frontrun on tr...   \n",
       "6     24  So The Secret Network Ethereum bridge is live....   \n",
       "7     29  Hi ethereum holders/investors/stakers etc.\\n\\n...   \n",
       "8     33  \\nIf you’ve been keeping up with Ripple then y...   \n",
       "9     35  I know the basics of crypto but I’m not the mo...   \n",
       "\n",
       "                                               title  subreddit  \\\n",
       "0       16 Ethereum Predictions From a Crypto Oracle          1   \n",
       "1                                   Upside of Ether?          1   \n",
       "2                                        i need help          1   \n",
       "3  New to this... is now a good time to invest in...          1   \n",
       "4                                    Should I stake?          1   \n",
       "5  We took a tour around the dark forest to see h...          1   \n",
       "6  More secretEther users needed! Privacy fans un...          1   \n",
       "7                                  Max supply of ETH          1   \n",
       "8                                  XRP right now lol          1   \n",
       "9                                  Is this a bubble?          1   \n",
       "\n",
       "                                                text  status_length  \\\n",
       "0  \\nhttpswwwcoindeskcom16ethereumpredictionscryp...             97   \n",
       "1  alright so ive been on the eth train for some ...            705   \n",
       "2  i tried almost a year and a half ago to get in...            816   \n",
       "3  hi guys i am not very familiar with ethereum b...            390   \n",
       "4  hi everyone i think this is the most suited su...            662   \n",
       "5  spoiler alert a lot  \\nwe go frontrun on trans...            396   \n",
       "6  so the secret network ethereum bridge is live ...            874   \n",
       "7  hi ethereum holdersinvestorsstakers etc\\n\\nis ...            142   \n",
       "8  \\nif youve been keeping up with ripple then yo...            272   \n",
       "9  i know the basics of crypto but im not the mos...            539   \n",
       "\n",
       "   word_count  \n",
       "0           7  \n",
       "1         142  \n",
       "2         156  \n",
       "3          76  \n",
       "4         129  \n",
       "5          46  \n",
       "6         124  \n",
       "7          27  \n",
       "8          42  \n",
       "9         105  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now that I have created a column of word counts and text lengths, I want to observe the distributions of each column and remove the outliers of those features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(posts['status_length'], posts['word_count']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105.71047639999999\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_means = []\n",
    "for i in range(10000):\n",
    "    s = np.random.choice(posts['word_count'], size = 500, replace = False) \n",
    "    x_bar = s.mean()\n",
    "    sample_means.append(x_bar)\n",
    "\n",
    "sns.histplot(sample_means)\n",
    "plt.axvline(np.mean(sample_means), color = 'r');\n",
    "print(np.mean(sample_means))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The above distribution is a random sampling from the word_count column. \n",
    "- We can observe an even distribution around the sample mean of 81 words. We should get rid of outliers ie rows that have word counts that exceed 3 standard deviations from the mean. \n",
    "- Below is a box plot of our distribution. As we can see, there is a lot of data that falls outside the 75% quartile. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(posts['word_count']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- But before cleaning for outliers, let's also take a look at a distribution of status length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "661.4169868\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_means = []\n",
    "for i in range(10000):\n",
    "    s = np.random.choice(posts['status_length'], size = 500, replace = False) \n",
    "    x_bar = s.mean()\n",
    "    sample_means.append(x_bar)\n",
    "\n",
    "sns.histplot(sample_means)\n",
    "plt.axvline(np.mean(sample_means), color = 'r');\n",
    "print(np.mean(sample_means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAEHCAYAAACQkJyuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAR8UlEQVR4nO3de3Bc5XnH8d+D5IRSEdcRLkNEGgHLH6VthmCVpJekZGoHG5iaTKCTv6y2mWZ6E2I64VIjY3usUbl2SpyZdGDKRG47JUMaLmOwG5sAmWk7EDlg7FhFXmRlQJiboAaD48rS0z/O0XYln7V2tZdnbb6fmR2tXr3veZ99V/rp7Nnds+buAgA03mnRBQDAhxUBDABBCGAACEIAA0AQAhgAgrRW0vmss87yzs7OOpUCAKemXbt2veXuS+e2VxTAnZ2dGhoaql1VAPAhYGY/y2rnEAQABCGAASAIAQwAQQhgAAhCAANAEAIYAIIQwAAQhAAGgCAEMAAEIYABIAgBDABBCGAACEIAA0AQAhgAghDAABCEAAaAIAQwAAQhgAEgCAEMAEEq+ky4am3evFlPP/20JKmjo0O5XE49PT2NLAEAmkZDAzifz+vNtyaklla9+c67jZwaAJpOQwNYktTSqqkz2hs+LQA0G44BA0AQAhgAghDAABCEAAaAIAQwAAQhgAEgCAEMAEEIYAAIQgADQBACGACCEMAAEIQABoAgBDAABCGAASAIAQwAQQhgAAhCAANAEAIYAIIQwAAQhAAGgCAEMAAEIYABIAgBDABBCGAACEIAA0AQAhgAghDAABCEAAaAIAQwAAQhgAEgCAEMAEEIYAAIQgADQBACGACCEMAAEIQABoAgBDAABCGAASAIAQwAQQhgAAhCAANAkNZGTLJ58+aK+/b09NSrHABoCg0J4Hw+X5e+AHAy4xAEAAQhgAEgCAEMAEEIYAAIQgADQBACGACCEMAAEIQABoAgBDAABCGAASAIAQwAQQhgAAhCAANAEAIYAIIQwAAQhAAGgCAEMAAEIYABIAgBDABBCGAACEIAA0AQAhgAghDAABCEAAaAIAQwAAQhgAEgCAEMAEEIYAAIQgADQBACGACCEMAAEIQABoAgBDAABCGAASAIAQwAQQhgAAhCAANAEAIYAIK0Rhcw1+7duyVJl112WWwhZWppadH09LTcXWamdevW6eGHH9Z1112nO+64Qy+//LJuuOEG3XXXXVq8eLFef/119fX16cEHH5SZadOmTXrnnXfU09Ojc845Ry0tLZqamtL4+LgkqaOjQ62trWppadG1116rTZs26bzzztPatWt19913a2pqSpOTk3rttde0efNmLVmyRDfddJPGx8d17rnn6rbbblN7e3uh3nw+r97eXt1zzz3K5XKSpImJCa1bt07urv7+/kL/iYkJbdy4UevXry+0DQ0N6cYbb9Sdd96pZcuWnXB8lqxtZrWVM65S5da5kLmKx0haUK21uI3IVs3a1vN+YQ+4SlNTU3J3SZK7a2BgQHv27FF/f79GRkZ05MgRDQwM6IMPPtDBgwc1PT2tgYEBDQ8Pa9++fdqyZYv6+/t15MgRjY6Oav/+/RodHdXRo0d19OhRjY6OamRkRMPDwxoYGJC7a3R0VP39/RoeHtbIyIgOHDigI0eOqL+/X4ODg8rn8zpy5Ij279+vLVu2zKq3v79f77//vvr7+wttg4OD2rdvn4aHh2f1Hxwc1J49e2a1bdiwQdPT04WgOdH4LFnbzGorZ1ylyq1zIXMVj1lorbW4jchWzdrW835pqgBevXp1dAlVO3bsmNxdY2Njs9rm9pnx2GOPzeo737ZnZI0ZGxvT1q1bZ7U9/vjjmpiYkJTs/c6MGxsbUz6f18TEhLZv317ov23bNk1MTBTa3V3bt2/XxMSEhoaGdPjwYUnS4cOHtWvXrpLjs2RtM6utnHGVKrfOhcxVPGbbtm3atm1bxbXW4jYiWzVrW+/7pSEBPD4+rnw+r3w+L01PJRP//N3Cw+GZy6FDhxpRTlOZG87Vmp6envX95ORk4T938V7vzPeDg4OanJw8rv/g4GBhW1NTU9qyZYs2bNgwa/z69etLjs+Stc2stnLGVarcOhcyV/GYycnJwn1aSa21uI3IVs3a1vt+mTeAzezrZjZkZkNvvvlmTSdHY+zYsUPS8XvNY2Nj2rlzZ+EQipQcRtmxY4d27txZCJJjx45px44dhb3fGYcPHy45PkvWNrPayhlXqXLrXMhcxWPcvTBPJbXW4jYiWzVrW+/7Zd4Advd73b3L3buWLl26oEk6OjqUy+WSJ31Oa5EkTZ/+MeVyOd1zzz2FC+pjxYoVkqTOzs5Z7Z2dnVq+fLnMrNBmZlqxYoWWL1+u1tbkOdrW1latWLFCbW1ts8a3tbWVHJ8la5tZbeWMq1S5dS5kruIxZlaYp5Jaa3Ebka2ata33/dJUx4AXL14cXULDzdy5tXLaabPv0kWLFmnNmjWSpL6+vlk/6+vrU3d3txYtWnRc/+7u7sK2WlpatGbNmuMOQWzcuLHk+CxZ28xqK2dcpcqtcyFzFY9ZtGhR4T6tpNZa3EZkq2Zt632/NFUAP/LII9ElVK21tVVmNmtvc27IFn9/5ZVXHrdneqJtz8ga09nZqauuumpW2xVXXFF46UwulyuM6+zsVC6XU3t7u1auXFnov2rVKrW3txfazUwrV65Ue3u7urq6CnvBbW1tWrZsWcnxWbK2mdVWzrhKlVvnQuYqHrNq1SqtWrWq4lprcRuRrZq1rff90nSvAz7ZzH0d8Nq1a+d9HfDatWsLrwNes2ZNzV4H3NfXpyVLlmjfvn2F1wHP/Y/d19en3t7eWXvD3d3dyufzcvdZ/bu7uzU2NjarbcOGDbrxxhu1cePGecdnydpmVls54ypVbp0LmWvumIXUWovbiGzVrG097xcrfmJiPl1dXT40NFTxJL29vYXrz+/dp6kzkv8iy84/+7hjvzN9OSYM4FRhZrvcvWtue1MdggCADxMCGACCEMAAEIQABoAgBDAABCGAASAIAQwAQQhgAAhCAANAEAIYAIIQwAAQhAAGgCAEMAAEIYABIAgBDABBCGAACEIAA0AQAhgAghDAABCEAAaAIAQwAAQhgAEgCAEMAEEIYAAIQgADQBACGACCEMAAEIQABoAgBDAABCGAASAIAQwAQQhgAAhCAANAEAIYAIIQwAAQhAAGgCAEMAAEIYABIEhrIybJ5XKSpHw+X3ZfADjVNSSAe3p6JEm9vb1l9wWAUx2HIAAgCAEMAEEIYAAIQgADQBACGACCEMAAEIQABoAgBDAABCGAASAIAQwAQQhgAAhCAANAEAIYAIIQwAAQhAAGgCAEMAAEIYABIAgBDABBCGAACEIAA0AQAhgAghDAABCEAAaAIAQwAAQhgAEgCAEMAEEIYAAIQgADQBACGACCEMAAEIQABoAgBDAABCGAASAIAQwAQQhgAAhCAANAEAIYAIIQwAAQhAAGgCAEMAAEaW34jFPH1PLBhCSTdHbDpweAZtHQAM7lchofH5ckdXR0KJfLNXJ6AGgq5u5ld+7q6vKhoaE6lgMApx4z2+XuXXPbOQYMAEEIYAAIQgADQBACGACCEMAAEIQABoAgBDAABCGAASAIAQwAQQhgAAhCAANAEAIYAIIQwAAQhAAGgCAEMAAEIYABIAgBDABBCGAACEIAA0AQAhgAglT0oZxm9qakny1wrrMkvbXAsY10MtR5MtQoUWetUWftNLrGT7n70rmNFQVwNcxsKOtTQZvNyVDnyVCjRJ21Rp210yw1cggCAIIQwAAQpJEBfG8D56rGyVDnyVCjRJ21Rp210xQ1NuwYMABgNg5BAEAQAhgAgtQ9gM1spZm9aGZ5M7u53vOVqGHMzPaY2fNmNpS2fdzMdpjZ/vTrkqL+f5PW+6KZXV7UvizdTt7MvmlmVmVd95vZG2a2t6itZnWZ2UfN7Ltp+zNm1lmjGjeY2Xi6ns+b2RWRNabb+aSZPWlmw2b2UzPrTdubbT1L1dk0a2pmp5vZs2a2O61xY5OuZak6m2Yt5+XudbtIapH0kqTzJX1E0m5JF9VzzhJ1jEk6a07bHZJuTq/fLOn29PpFaZ0flXReWn9L+rNnJf2WJJO0TdKqKuv6gqRLJO2tR12S/kLSP6TXvyrpuzWqcYOkb2T0DakxHXuOpEvS62dKGknrabb1LFVn06xpur229PoiSc9I+lwTrmWpOptmLee71HsP+FJJeXcfdff/lfSApNV1nrNcqyUNptcHJV1d1P6Aux919wOS8pIuNbNzJH3M3f/Lk3tjS9GYBXH3H0l6u451FW/re5J+f+Y/e5U1lhJSY1rnQXf/SXr9PUnDkjrUfOtZqs5SGl6nJw6n3y5KL67mW8tSdZYS9vtZSr0DuEPSy0Xfv6IT/7LVi0v6gZntMrOvp21nu/tBKfmjkPTLaXupmjvS63Pba62WdRXGuPsxSYcktdeozr8ysxcsOUQx81C0KWpMHyZ+RskeUdOu55w6pSZaUzNrMbPnJb0haYe7N+ValqhTaqK1PJF6B3DWf4qI1739jrtfImmVpL80sy+coG+pmqNvy0LqqlfN35Z0gaSLJR2UdPc88zWsRjNrk/Rvkq5393dP1LXEvA2pNaPOplpTd59y94slnatkL/HXT9A9bC1L1NlUa3ki9Q7gVyR9suj7cyW9Wuc5j+Pur6Zf35D0kJJDI6+nDz2Ufn0j7V6q5lfS63Pba62WdRXGmFmrpMUq/3BCSe7+evqLPy3pPiXrGV6jmS1SEmr/4u7fT5ubbj2z6mzWNXX3/5H0lKSVasK1zKqzWdcyS70D+MeSLjSz88zsI0oOYj9a5zlnMbNfNLMzZ65L+pKkvWkd3Wm3bkmPpNcflfTV9NnP8yRdKOnZ9CHXe2b2ufQY0JqiMbVUy7qKt3WNpB+mx7iqMvNHmPqykvUMrTHd7j9KGnb3vyv6UVOtZ6k6m2lNzWypmf1Sev0XJC2X9N9qvrXMrLOZ1nJeC332rtyLpCuUPNP7kqRb6j1fxvznK3nmc7ekn87UoOQ4zhOS9qdfP1405pa03hdV9EoHSV3pnfmSpG8pfSdhFbX9q5KHSJNK/tN+rZZ1STpd0oNKnmx4VtL5NarxnyTtkfSCkl/QcyJrTLfzu0oeGr4g6fn0ckUTrmepOptmTSV9WtJzaS17Jd1a67+ZGq1lqTqbZi3nu/BWZAAIwjvhACAIAQwAQQhgAAhCAANAEAIYAIIQwAAQhABGXZjZ9WZ2Rq36VTDvZWa2tVbbK9ru1WZ2UdH3T5lZ+Kfq4uRGAKNerpdUTrCW2y/a1UpOZwjUDAGMqqVv937MkhNj7zWz9ZI+IelJM3sy7fNtMxuy2SfOvi6j3+Gi7V5jZt9Jr1+bbnu3mf2ogrruN7Mfm9lzZrY6bf8jM/u+mW235OTidxSN+ZqZjaR7uPeZ2bfM7Lcl/YGkOy05wfcFafdrLTkh+IiZfb66VcSHUWt0ATglrJT0qrtfKUlmtljSH0v6oru/lfa5xd3fNrMWSU+Y2afd/Ztm9tdz+pVyq6TL3X185v3/ZbhFyXv3/yQd86yZ7Ux/drGSU0EelfSimW2WNCVpnZIT0L8n6YeSdrv7f5rZo5K2uvv30tsoSa3ufqkln7iwXsm5CICysQeMWtgjabmZ3W5mn3f3Qxl9/tDMfqLkvfu/psofzv+HpO+Y2Z8q+aSVcnxJ0s2WnC/2KSXv6/+V9GdPuPshd/+5pH2SPqXkrFlPu/vb7j6p5BwAJzJzxrVdkjrLrAkoYA8YVXP3ETNbpuSkMn9rZj8o/nl65qlvSPpNd38nPaxweqnNFV0v9HH3PzOzz0q6UtLzZnaxu0/MU5pJ+oq7vzinns8q2fOdMaXkb6HSTzqY2cbMeKAi7AGjamb2CUkfuPs/S7pL//8Q/sy0y8ckvS/pkJmdreTE+DOK+0nJOWd/1cxOU3IqwZk5LnD3Z9z9VklvafZ5XUv5d0k96SkGZWafmaf/s5J+z8yWWHLu16+coE6gavzXRi38hpInqKaVnLbyz5V8wOE2Mzvo7l80s+eUnA50VMnhhBn3FvdT8mGPW5V8DMxeSW1pvzvN7EIle6lPKDm96Hw2Sfp7SS+kITwm6apSndPjywNKPiLoVSWHJmYOpzwg6b70icNrypgbmBenowSKmFmbux9O94AfknS/uz8UXRdOTRyCAGbbkD5pt1fSAUkPh1aDUxp7wDgpmdnlkm6f03zA3b+c1R9oRgQwAAThEAQABCGAASAIAQwAQQhgAAjyf4fawHSxF07YAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(posts['status_length']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's get rid of the data that fall more than 3 standard deviations away. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#posts.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "      <th>status_length</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\\nhttps://www.coindesk.com/16-ethereum-predict...</td>\n",
       "      <td>16 Ethereum Predictions From a Crypto Oracle</td>\n",
       "      <td>1</td>\n",
       "      <td>\\nhttpswwwcoindeskcom16ethereumpredictionscryp...</td>\n",
       "      <td>97</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>Alright so I’ve been on the ETH train for some...</td>\n",
       "      <td>Upside of Ether?</td>\n",
       "      <td>1</td>\n",
       "      <td>alright so ive been on the eth train for some ...</td>\n",
       "      <td>705</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>i tried almost a year and a half ago to get in...</td>\n",
       "      <td>i need help</td>\n",
       "      <td>1</td>\n",
       "      <td>i tried almost a year and a half ago to get in...</td>\n",
       "      <td>816</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>Hi guys, I am not very familiar with ethereum ...</td>\n",
       "      <td>New to this... is now a good time to invest in...</td>\n",
       "      <td>1</td>\n",
       "      <td>hi guys i am not very familiar with ethereum b...</td>\n",
       "      <td>390</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>Hi everyone, I think this is the most suited s...</td>\n",
       "      <td>Should I stake?</td>\n",
       "      <td>1</td>\n",
       "      <td>hi everyone i think this is the most suited su...</td>\n",
       "      <td>662</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           selftext  \\\n",
       "0      1  \\nhttps://www.coindesk.com/16-ethereum-predict...   \n",
       "1      8  Alright so I’ve been on the ETH train for some...   \n",
       "2     15  i tried almost a year and a half ago to get in...   \n",
       "3     16  Hi guys, I am not very familiar with ethereum ...   \n",
       "4     21  Hi everyone, I think this is the most suited s...   \n",
       "\n",
       "                                               title  subreddit  \\\n",
       "0       16 Ethereum Predictions From a Crypto Oracle          1   \n",
       "1                                   Upside of Ether?          1   \n",
       "2                                        i need help          1   \n",
       "3  New to this... is now a good time to invest in...          1   \n",
       "4                                    Should I stake?          1   \n",
       "\n",
       "                                                text  status_length  \\\n",
       "0  \\nhttpswwwcoindeskcom16ethereumpredictionscryp...             97   \n",
       "1  alright so ive been on the eth train for some ...            705   \n",
       "2  i tried almost a year and a half ago to get in...            816   \n",
       "3  hi guys i am not very familiar with ethereum b...            390   \n",
       "4  hi everyone i think this is the most suited su...            662   \n",
       "\n",
       "   word_count  \n",
       "0           7  \n",
       "1         142  \n",
       "2         156  \n",
       "3          76  \n",
       "4         129  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.51284354 0.18854494 0.26128153 ... 0.21670173 0.1699425  0.24049965]\n"
     ]
    }
   ],
   "source": [
    "# z-scores\n",
    "z = np.abs(stats.zscore(posts['word_count']))\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([  263,   304,   305,   529,   584,   629,   666,   734,   834,\n",
       "           861,   867,   873,   878,   886,   887,   941,   945,   963,\n",
       "          1010,  1076,  1229,  1423,  1485,  1587,  1619,  1634,  1740,\n",
       "          1742,  1800,  2052,  2064,  2103,  2131,  2177,  2278,  2310,\n",
       "          2325,  2330,  2372,  2374,  2430,  2465,  2471,  2483,  2488,\n",
       "          2529,  2534,  2645,  2665,  2671,  2698,  2712,  2719,  2745,\n",
       "          2748,  2756,  2757,  2768,  2787,  2833,  2855,  2876,  2893,\n",
       "          2899,  2999,  3055,  3060,  3176,  3227,  3284,  3338,  3368,\n",
       "          3373,  3383,  3426,  3433,  3444,  3464,  3602,  3605,  3621,\n",
       "          3656,  3804,  3811,  3843,  3869,  3905,  3927,  3946,  3997,\n",
       "          4000,  4003,  4060,  4066,  4069,  4084,  4105,  4107,  4122,\n",
       "          4129,  4135,  4148,  4155,  4170,  4202,  4206,  4208,  4268,\n",
       "          4303,  4362,  4378,  4423,  4444,  4488,  4507,  4572,  4579,\n",
       "          4580,  4582,  4589,  4617,  4620,  4640,  4643,  4721,  4782,\n",
       "          4818,  4835,  4907,  4918,  5169,  5171,  5244,  5344,  5359,\n",
       "          5398,  5461,  5462,  5508,  5551,  5602,  5720,  5750,  5774,\n",
       "          6056,  6094,  6214,  6236,  6845,  7014,  7018,  7033,  7068,\n",
       "          7169,  7274,  7293,  7346,  7761,  7950,  8236,  8307,  8364,\n",
       "          8464,  8536,  8704,  9351,  9388,  9424,  9659,  9673,  9795,\n",
       "          9897,  9922, 10251, 10618], dtype=int64),)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create list of z_scores whose values are greater than 3\n",
    "outliers = [] \n",
    "outliers.append(np.where(z>3))\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 175)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many 'word_count' outliers\n",
    "range(len(outliers[0][0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "926"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example outliers\n",
    "posts['word_count'][263]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[926, 825, 825, 1204, 724, 1902, 1267, 2046, 2439, 3406]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create list of all outlier values for word_count\n",
    "outliers_list = []\n",
    "for index in (outliers[0][0]):\n",
    "    #print(index)\n",
    "    outliers_list.append(posts['word_count'][index])\n",
    "outliers_list[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we want to get rid of all the values from this list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for outliers for word count because we want to take words out first \n",
    "\n",
    "posts = posts[(np.abs(stats.zscore(posts['word_count'])) < 3)]\n",
    "posts = posts[(np.abs(stats.zscore(posts['status_length'])) < 3)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(posts['word_count'], whis=1.9946);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAEHCAYAAACDR9xaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPNElEQVR4nO3dbWxd9X3A8e/P8VNJyFYnAYHJAyGRNiYmCqzdUzuQqlIgGqC2U1/Btmpl05YFTbzIFK0kQtpE2dAglSqBhlroNKRtZUOh3TooXaVtKk1KgBQKmEAIhpEHT4RAsGPy34t7bG6CncT2ffjZ/n4kKzfH5+l/j/PN8bn2uVFKQZKUT0e7d0CSNDEDLUlJGWhJSspAS1JSBlqSkuqcysxLly4tq1atatKuSNLctGPHjgOllGVTXW5KgV61ahXbt2+f6jYkaV6LiD3TWc5LHJKUlIGWpKQMtCQlZaAlKSkDLUlJGWhJSspAS1JSBlqSkjLQkpSUgZakpAy0JCVloCUpKQMtSUkZaElKykBLUlIGWpKSMtCSlJSBlqSkDLQkJTWl9yTMauvWrQwMDDRsfYODgwD09/c3bJ3TsWbNGtavX9/WfZDUPnMi0AMDA+zc9Rzvn9HXkPUtePctAP53uH1Pz4J3h9q2bUk5zIlAA7x/Rh9HfuHqhqzrIz/7DkDD1jeTfZA0f3kNWpKSMtCSlJSBlqSkDLQkJWWgJSkpAy1JSRloSUrKQEtSUgZakpIy0JKUlIGWpKQMtCQlZaAlKSkDLUlJGWhJSspAS1JSBlqSkjLQkpSUgZakpAy0JCVloCUpKQMtSUkZaElKykBLUlIGWpKSMtCSlJSBlqSkDLQkJWWgJSkpAy1JSRloSUrKQEtSUgZakpIy0JKUlIGWpKQMtCQlZaAlKSkDLUlJGWhJSspAS1JSBlqSkmpJoLdu3crWrVtbsSmppfzaVjN1tmIjAwMDrdiM1HJ+bauZvMQhSUkZaElKykBLUlIGWpKSMtCSlJSBlqSkDLQkJWWgJSkpAy1JSRloSUrKQEtSUgZakpIy0JKUlIGWpKQMtCQlZaAlKSkDLUlJGWhJSspAS1JSBlqSkjLQkpSUgZakpAy0JCVloCUpKQMtSUkZaElKykBLUlIGWpKSMtCSlJSBlqSkDLQkJWWgJSkpAy1JSRloSUrKQEtSUgZakpIy0JKUlIGWpKQ6270D0mw3NDTE5ZdfPuXlIoJSCgsXLuSdd95p6D6NrbteT08PCxcuZGhoaNJ5JtLd3c2yZcsYHBwcn7ZixQp6enoYHh5m7969lFLo6Ojg2LFjAPT19TE0NHTcNiKCrq4uzjvvPAAGBwcZHh4GYPHixRw6dIglS5Zw8OBBli1bxv79++ns7GR0dJSuri4igpGREVauXMmRI0fYt28fXV1ddHR0EBEsX76cm266iY0bNzI6Osq6det49NFHAVi0aBEHDhwY3//Ozk46OjoYGRkhIjj33HPZt28fR48eZeXKldx5550AbNmyheuvv57bbruNO+64g0svvXRax2O6DLQ0Q3v37p3WcmPhanSc69ddb3h4eDyIk80zkZGRkePiDPDqq69+aL6xOAPj/wnUb6OUwsjICLt37/7QsocOHQLg4MGDAOzfvx+A0dFRAI4ePTo+7549e8Yf109/8cUX2bx58/gy27ZtG//ce++9d9z2xuYZ26/68e3Zs4f777+fUgrPPPMMzz77LMeOHePWW289bp2tYKClGRgLkXI4fPhwQ9bzyCOPjJ/9j8X88OHD7Nixo6Vn0S0J9ODgIEeOHGHDhg1NWf/AwAAdI6d3NjBbdLx3iIGBt5v2nKkxpnv2rNxGR0eJiA9Nb/VZ9ClfJIyIL0fE9ojYPvZthyTNdRNdAmrUGfrpOuUZdCnlHuAegMsuu2xap6n9/f0A3HXXXdNZ/JQ2bNjAjt1vNmXd7XKsdzFrVp/dtOdMjTGdFwc1O0z0IuqiRYtaug/+mJ00A8uXL2/3LqgJOjs76ez88Pnrli1bWrofBlqagb6+vnbvguo06gz3mmuu4aqrriIixkO9aNGilv+YnYGWZmi6Z9FjL0ItXLiwkbtz3Lrr9fT0HPcfykTzTKS7u3v8MuWYFStWsHbtWlasWDG+no6OD3Iytp36bUQE3d3drF69mtWrV9PT0zP+ucWLFwOwZMkSAJYtWwYwHseuri66u7sBWLlyJWedddb49J6eHnp7e1m7di2bN28eX2bdunX09vbS29vL0qVLj9v/zs7O8fVFBP39/XR1dY2v/4YbbuDGG2/koosuYtOmTXR0dLT87Bn8MTtpxvr6+njggQfavRuqjP1yCsAtt9wyo3XdfffdAFxxxRUzWs90eQYtSUkZaElKykBLUlIGWpKSMtCSlJSBlqSkDLQkJWWgJSkpAy1JSRloSUrKQEtSUgZakpIy0JKUlIGWpKQMtCQlZaAlKSkDLUlJGWhJSspAS1JSBlqSkjLQkpSUgZakpAy0JCVloCUpKQMtSUkZaElKykBLUlIGWpKSMtCSlJSBlqSkDLQkJWWgJSkpAy1JSRloSUrKQEtSUgZakpIy0JKUlIGWpKQ6W7GRNWvWtGIzUsv5ta1makmg169f34rNSC3n17aayUsckpSUgZakpAy0JCVloCUpKQMtSUkZaElKykBLUlIGWpKSMtCSlJSBlqSkDLQkJWWgJSkpAy1JSRloSUrKQEtSUgZakpIy0JKUlIGWpKQMtCQlZaAlKSkDLUlJGWhJSspAS1JSBlqSkjLQkpSUgZakpAy0JCVloCUpKQMtSUkZaElKykBLUlIGWpKSMtCSlJSBlqSkDLQkJWWgJSkpAy1JSRloSUrKQEtSUgZakpLqbPcONMqCd4f4yM++06B1HQRo2Pqmtw9DwNlt276k9psTgV6zZk1D1zc4OApAf387A3l2w8claXaZE4Fev359u3dBkhrOa9CSlJSBlqSkDLQkJWWgJSkpAy1JSRloSUrKQEtSUgZakpIy0JKUlIGWpKQMtCQlZaAlKSkDLUlJGWhJSspAS1JSBlqSkjLQkpSUgZakpAy0JCVloCUpqSilnP7MEfuBPdPYzlLgwDSWmwvm89hhfo/fsc9PE419ZSll2VRXNKVAT1dEbC+lXNb0DSU0n8cO83v8jt2xz5SXOCQpKQMtSUm1KtD3tGg7Gc3nscP8Hr9jn58aNvaWXIOWJE2dlzgkKSkDLUlJNT3QEfHZiHg+IgYiYmOzt9cOEfFKRDwTETsjYns1rS8i/iMiXqz+/Gjd/H9ePR/PR8SV7dvzqYuI+yJiX0Tsqps25bFGxKXVczYQEXdHRLR6LFM1ydg3R8Rgdex3RsTVdZ+bS2NfHhGPR8RzEfHTiNhQTZ/zx/4kY2/+sS+lNO0DWAC8BKwGuoGngAubuc12fACvAEtPmPZVYGP1eCNwe/X4wup56AHOr56fBe0ewxTG+ingEmDXTMYKPAH8GhDAd4Gr2j22aY59M3DLBPPOtbGfA1xSPT4TeKEa45w/9icZe9OPfbPPoD8ODJRSdpdSRoAHgWubvM0srgW+WT3+JnBd3fQHSynDpZSXgQFqz9OsUEr5ITB0wuQpjTUizgEWl1L+p9S+au+vWyatScY+mbk29jdKKT+pHr8NPAf0Mw+O/UnGPpmGjb3Zge4H9tb9/TVOPrDZqgDfi4gdEfHlatrZpZQ3oHaAgbOq6XPxOZnqWPurxydOn63+JCKeri6BjH2LP2fHHhGrgI8BP2KeHfsTxg5NPvbNDvRE11fm4s/1/UYp5RLgKuCPI+JTJ5l3vjwnMPlY59Jz8HXgAuBi4A3gb6rpc3LsEbEI+Gfg5lLKoZPNOsG0WT3+Ccbe9GPf7EC/Biyv+/t5wOtN3mbLlVJer/7cBzxE7ZLFm9W3NFR/7qtmn4vPyVTH+lr1+MTps04p5c1SyvullGPAvXxwuWrOjT0iuqgF6u9LKd+uJs+LYz/R2Ftx7Jsd6B8DayPi/IjoBr4IPNzkbbZURCyMiDPHHgOfAXZRG+eN1Ww3Av9aPX4Y+GJE9ETE+cBaai8czGZTGmv1rfDbEfGr1avYN9QtM6uMxalyPbVjD3Ns7NW+/h3wXCnlzrpPzfljP9nYW3LsW/AK6NXUXvV8CdjU7ldkmzC+1dResX0K+OnYGIElwGPAi9WffXXLbKqej+dJ/gr2BOP9B2rfzh2ldkbwpemMFbis+oJ+Cfga1W+1Zv6YZOwPAM8AT1f/MM+Zo2P/TWrfjj8N7Kw+rp4Px/4kY2/6sfdXvSUpKX+TUJKSMtCSlJSBlqSkDLQkJWWgJSkpAy1JSRloNUVE3BwRZzRqvils9/KI2Nao9dWt97qIuLDu7z+IiHn5rtVqHQOtZrkZOJ3wnu587XYdtdtISi1joDVj1a+7PxIRT0XEroi4FTgXeDwiHq/m+XpEbK9ueL6lmvanE8x3uG69n4+Ib1SPv1Ct+6mI+OEU9uu+iPhxRDwZEddW0383Ir4dEf8WtRvNf7VumS9FxAvVGfK9EfG1iPh14LeBO6obs19Qzf6FiHiimv+TM3sWpQ/rbPcOaE74LPB6KeUagIj4OeD3gCtKKQeqeTaVUoYiYgHwWET8cinl7oj4sxPmm8xXgCtLKYMR8fOnuV+bgO+XUn6/WuaJiHi0+tzF1G4bOQw8HxFbgfeBv6B2U/63ge8DT5VS/jsiHga2lVL+qRojQGcp5eNReyeNW4FPn+Z+SafFM2g1wjPApyPi9oj4ZCnlrQnm+Z2I+AnwJPBLTP1ywX8B34iIP6D2Tj2n4zPAxojYCfwA6AVWVJ97rJTyVinlPeBZYCW1u5H9ZyllqJRyFPjHU6x/7I5uO4BVp7lP0mnzDFozVkp5ISIupXYDmb+KiO/Vf766o9ctwK+UUv6vumzRO9nq6h6Pz1NK+cOI+ARwDbAzIi4upRw8xa4F8LlSyvMn7M8nqJ05j3mf2r+Fqb433tg6xpaXGsozaM1YRJwLvFtK+Rbw13xwieDMapbFwDvAWxFxNrU3NhhTPx/U7i/8ixHRQe0WjmPbuKCU8qNSyleAAxx/v93J/Duwvrq1IxHxsVPM/wTwWxHx0YjoBD53kv2Ums7/9dUIF1F7Ae0YtVtx/hG1N8b8bkS8UUq5IiKepHY71t3ULleMuad+PmpvPLqN2lsG7QIWVfPdERFrqZ3lPkbt9q6nchvwt8DTVaRfAdZNNnN1ffsvqb2d0evULn2MXa55ELi3emHz86exbWnGvN2oVCciFpVSDldn0A8B95VSHmr3fml+8hKHdLzN1YuKu4CXgX9p695oXvMMWrNSRFwJ3H7C5JdLKddPNL80GxloSUrKSxySlJSBlqSkDLQkJWWgJSmp/wfN/L1ptc8xtQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(posts['status_length'], whis=1.9946);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Box plots look much better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10213, 7)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.to_csv('./datasets/posts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we got ride of over 600 outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These subreddits have a lot of integers. It's the nature of finance. Some of them are useful Ethereum addresses, most of which are denoted by the first two characters being '0x'. I want to take out all the integers from the trainset because I don't think numbers will help the model predict whether the text is from a Bitcoin or an Ethereum subreddit. However, I want to keep the ETH addresses because this will most likely improve the model's predictions for Ethereum subreddits.\n",
    "\n",
    "- Using RegEx I will find all of the texts that begin with '0x' and all of the texts that are integers \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I want to lemmatize my text data so that my model doesn't learn from redundancies. For example, instead of the model learning 'happy', 'happiness', and 'happily,' I want to create a base word, in this case 'happy,' for these three because they have similar meanings. Therefore the model will only learn from happy. \n",
    "- Below is code to briefly demonstrate how lemmatizing the data will yield fewer text words for when I CountVectorize my data. It is not important to the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10213 entries, 0 to 10631\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   index          10213 non-null  int64 \n",
      " 1   selftext       10213 non-null  object\n",
      " 2   title          10213 non-null  object\n",
      " 3   subreddit      10213 non-null  int64 \n",
      " 4   text           10213 non-null  object\n",
      " 5   status_length  10213 non-null  int64 \n",
      " 6   word_count     10213 non-null  int64 \n",
      "dtypes: int64(4), object(3)\n",
      "memory usage: 638.3+ KB\n"
     ]
    }
   ],
   "source": [
    "posts.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Experimentation: RegEx\n",
    "\n",
    "- code in this section is an exploration of tokenized data \n",
    "- it will not be used in the model \n",
    "- instead I will call the Lemmatizer in my CountVectorizer when I create my model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Regex\n",
    "tokenizer = RegexpTokenizer(r'\\w+') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alright',\n",
       " 'so',\n",
       " 'ive',\n",
       " 'been',\n",
       " 'on',\n",
       " 'the',\n",
       " 'eth',\n",
       " 'train',\n",
       " 'for',\n",
       " 'some',\n",
       " 'time',\n",
       " 'now',\n",
       " 'sold',\n",
       " 'on',\n",
       " 'the',\n",
       " 'concept',\n",
       " 'and',\n",
       " 'think',\n",
       " 'the',\n",
       " 'future',\n",
       " 'is',\n",
       " 'very',\n",
       " 'bright',\n",
       " 'what',\n",
       " 'i',\n",
       " 'dont',\n",
       " 'know',\n",
       " 'is',\n",
       " 'there',\n",
       " 'any',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'realistic',\n",
       " 'way',\n",
       " 'to',\n",
       " 'arrive',\n",
       " 'at',\n",
       " 'a',\n",
       " 'best',\n",
       " 'case',\n",
       " 'scenario',\n",
       " 'price',\n",
       " 'valuation',\n",
       " 'for',\n",
       " 'eth',\n",
       " 'for',\n",
       " 'example',\n",
       " 'if',\n",
       " 'btc',\n",
       " 'can',\n",
       " 'gain',\n",
       " 'equal',\n",
       " 'footing',\n",
       " 'with',\n",
       " 'gold',\n",
       " 'from',\n",
       " 'a',\n",
       " 'market',\n",
       " 'cap',\n",
       " 'perspective',\n",
       " 'we',\n",
       " 'could',\n",
       " 'roughly',\n",
       " 'expect',\n",
       " 'a',\n",
       " '500k',\n",
       " 'btc',\n",
       " 'sure',\n",
       " 'there',\n",
       " 'is',\n",
       " 'plenty',\n",
       " 'of',\n",
       " 'variance',\n",
       " 'up',\n",
       " 'or',\n",
       " 'down',\n",
       " 'based',\n",
       " 'on',\n",
       " 'a',\n",
       " 'variety',\n",
       " 'pack',\n",
       " 'of',\n",
       " 'factors',\n",
       " 'but',\n",
       " 'i',\n",
       " 'can',\n",
       " 'get',\n",
       " 'a',\n",
       " 'rough',\n",
       " 'idea',\n",
       " 'of',\n",
       " 'what',\n",
       " 'it',\n",
       " 'could',\n",
       " 'be',\n",
       " 'worth',\n",
       " 'is',\n",
       " 'there',\n",
       " 'anything',\n",
       " 'we',\n",
       " 'can',\n",
       " 'look',\n",
       " 'at',\n",
       " 'to',\n",
       " 'estimate',\n",
       " 'the',\n",
       " 'best',\n",
       " 'case',\n",
       " 'price',\n",
       " 'scenario',\n",
       " 'for',\n",
       " 'eth',\n",
       " 'are',\n",
       " 'you',\n",
       " 'all',\n",
       " 'in',\n",
       " 'the',\n",
       " 'same',\n",
       " 'boat',\n",
       " 'as',\n",
       " 'me',\n",
       " 'where',\n",
       " 'i',\n",
       " 'believe',\n",
       " 'in',\n",
       " 'ethereum',\n",
       " 'and',\n",
       " 'just',\n",
       " 'blindly',\n",
       " 'hope',\n",
       " 'the',\n",
       " 'value',\n",
       " 'goes',\n",
       " 'up',\n",
       " 'as',\n",
       " 'adoption',\n",
       " 'and',\n",
       " 'btc',\n",
       " 'price',\n",
       " 'increaseupside',\n",
       " 'of',\n",
       " 'ether']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_tokens = []\n",
    "for i in posts['text']: \n",
    "    posts_tokens.append(tokenizer.tokenize(i))\n",
    "posts_tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alright', 'so', 'ive', 'been', 'on', 'the', 'eth', 'train', 'for', 'some', 'time', 'now', 'sold', 'on', 'the', 'concept', 'and', 'think', 'the', 'future', 'is', 'very', 'bright', 'what', 'i', 'dont', 'know', 'is', 'there', 'any', 'sort', 'of', 'realistic', 'way', 'to', 'arrive', 'at', 'a', 'best', 'case', 'scenario', 'price', 'valuation', 'for', 'eth', 'for', 'example', 'if', 'btc', 'can', 'gain', 'equal', 'footing', 'with', 'gold', 'from', 'a', 'market', 'cap', 'perspective', 'we', 'could', 'roughly', 'expect', 'a', '500k', 'btc', 'sure', 'there', 'is', 'plenty', 'of', 'variance', 'up', 'or', 'down', 'based', 'on', 'a', 'variety', 'pack', 'of', 'factors', 'but', 'i', 'can', 'get', 'a', 'rough', 'idea', 'of', 'what', 'it', 'could', 'be', 'worth', 'is', 'there', 'anything', 'we', 'can', 'look', 'at', 'to', 'estimate', 'the', 'best', 'case', 'price', 'scenario', 'for', 'eth', 'are', 'you', 'all', 'in', 'the', 'same', 'boat', 'as', 'me', 'where', 'i', 'believe', 'in', 'ethereum', 'and', 'just', 'blindly', 'hope', 'the', 'value', 'goes', 'up', 'as', 'adoption', 'and', 'btc', 'price', 'increaseupside', 'of', 'ether']\n"
     ]
    }
   ],
   "source": [
    "print(posts_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate lemmatizer.\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['im', 'newer', 'to', 'crypto', 'and']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_text_tokens = [lemmatizer.lemmatize(i) for i in posts_tokens[12]]\n",
    "lem_text_tokens[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['httpswwwcoindeskcom16ethereumpredictionscryptooracle16',\n",
       " 'ethereum',\n",
       " 'prediction',\n",
       " 'from',\n",
       " 'a']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_lem = []\n",
    "tokens = []\n",
    "for i in range(len(posts_tokens)):\n",
    "    for j in posts_tokens[i]: \n",
    "        tokens.append(j)\n",
    "        token_lem.append(lemmatizer.lemmatize(j))\n",
    "token_lem[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('httpswwwcoindeskcom16ethereumpredictionscryptooracle16',\n",
       "  'httpswwwcoindeskcom16ethereumpredictionscryptooracle16'),\n",
       " ('ethereum', 'ethereum'),\n",
       " ('predictions', 'prediction'),\n",
       " ('from', 'from'),\n",
       " ('a', 'a')]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(tokens, token_lem))[0:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Above we can see the difference between the lemma and the text data. Not all the words are different, but some of them are which reduces the amount of different variations the same meaning can be represented in text. it might be worth examining differences in model performance given lemma and stems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've cleaned the data of all upper case letters, special characters, and outliers. \n",
    "\n",
    "Now I want to check out most frequent words to create a short stop words list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a CountVectorizer with stopwords = 'TPDNM'\n",
    "# used from sklearn documentation https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "# it allows me to call LemmaTokenizer() with my Pipeline()\n",
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "         self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "         # return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if t not in stopwords.words('english')]\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "\n",
    "#cvec = CountVectorizer(tokenizer=LemmaTokenizer(), stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before I model my data, I want to CountVectorize my texts to see the most common words in the data. They might be too telling of which subreddit post the text came from and should be included in my stop_words list when I instantiate my transformers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = posts['text']\n",
    "y = posts['subreddit']\n",
    "\n",
    "# Split the data into the training and testing sets for a prelim CountVectorizer\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)\n",
    "\n",
    "cvec = CountVectorizer()\n",
    "cvec.fit(X_train)\n",
    "X_train = cvec.transform(X_train)\n",
    "X_train_df = pd.DataFrame(X_train.todense(), \n",
    "                          columns=cvec.get_feature_names())\n",
    "X_train_df.sum().sort_values(ascending=False).head(20).plot(kind='barh');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most of the most common words seem like they will be included in the english stop words list. However, words like 'bitcoin' and 'ethereum' might be too much of a giveaway for predicting which subreddit a test came from. I will also include 'btc' and 'eth' which are abbreviations for bitcoin and ethereum since they are also commonly used for both assets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add stop words to english stop words list\n",
    "\n",
    "my_additional_stop_words = ['btc', 'eth', 'ethereum', 'bitcoin' ]\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)\n",
    "\n",
    "# source: https://stackoverflow.com/questions/24386489/adding-words-to-scikit-learns-countvectorizers-stop-list/24386751"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a sentiment column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "def get_compound_sentiment(post):\n",
    "    return sia.polarity_scores(post)['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts['sentiment'] = posts['text'].apply(get_compound_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "      <th>status_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\\nhttps://www.coindesk.com/16-ethereum-predict...</td>\n",
       "      <td>16 Ethereum Predictions From a Crypto Oracle</td>\n",
       "      <td>1</td>\n",
       "      <td>\\nhttpswwwcoindeskcom16ethereumpredictionscryp...</td>\n",
       "      <td>97</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>Alright so I’ve been on the ETH train for some...</td>\n",
       "      <td>Upside of Ether?</td>\n",
       "      <td>1</td>\n",
       "      <td>alright so ive been on the eth train for some ...</td>\n",
       "      <td>705</td>\n",
       "      <td>142</td>\n",
       "      <td>0.9724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>i tried almost a year and a half ago to get in...</td>\n",
       "      <td>i need help</td>\n",
       "      <td>1</td>\n",
       "      <td>i tried almost a year and a half ago to get in...</td>\n",
       "      <td>816</td>\n",
       "      <td>156</td>\n",
       "      <td>0.9355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>Hi guys, I am not very familiar with ethereum ...</td>\n",
       "      <td>New to this... is now a good time to invest in...</td>\n",
       "      <td>1</td>\n",
       "      <td>hi guys i am not very familiar with ethereum b...</td>\n",
       "      <td>390</td>\n",
       "      <td>76</td>\n",
       "      <td>0.9109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>Hi everyone, I think this is the most suited s...</td>\n",
       "      <td>Should I stake?</td>\n",
       "      <td>1</td>\n",
       "      <td>hi everyone i think this is the most suited su...</td>\n",
       "      <td>662</td>\n",
       "      <td>129</td>\n",
       "      <td>0.9294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23</td>\n",
       "      <td>Spoiler alert.. a lot.  \\nWe go frontrun on tr...</td>\n",
       "      <td>We took a tour around the dark forest to see h...</td>\n",
       "      <td>1</td>\n",
       "      <td>spoiler alert a lot  \\nwe go frontrun on trans...</td>\n",
       "      <td>396</td>\n",
       "      <td>46</td>\n",
       "      <td>0.7269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>24</td>\n",
       "      <td>So The Secret Network Ethereum bridge is live....</td>\n",
       "      <td>More secretEther users needed! Privacy fans un...</td>\n",
       "      <td>1</td>\n",
       "      <td>so the secret network ethereum bridge is live ...</td>\n",
       "      <td>874</td>\n",
       "      <td>124</td>\n",
       "      <td>0.9801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>29</td>\n",
       "      <td>Hi ethereum holders/investors/stakers etc.\\n\\n...</td>\n",
       "      <td>Max supply of ETH</td>\n",
       "      <td>1</td>\n",
       "      <td>hi ethereum holdersinvestorsstakers etc\\n\\nis ...</td>\n",
       "      <td>142</td>\n",
       "      <td>27</td>\n",
       "      <td>0.1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>33</td>\n",
       "      <td>\\nIf you’ve been keeping up with Ripple then y...</td>\n",
       "      <td>XRP right now lol</td>\n",
       "      <td>1</td>\n",
       "      <td>\\nif youve been keeping up with ripple then yo...</td>\n",
       "      <td>272</td>\n",
       "      <td>42</td>\n",
       "      <td>0.8909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>35</td>\n",
       "      <td>I know the basics of crypto but I’m not the mo...</td>\n",
       "      <td>Is this a bubble?</td>\n",
       "      <td>1</td>\n",
       "      <td>i know the basics of crypto but im not the mos...</td>\n",
       "      <td>539</td>\n",
       "      <td>105</td>\n",
       "      <td>0.7301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           selftext  \\\n",
       "0      1  \\nhttps://www.coindesk.com/16-ethereum-predict...   \n",
       "1      8  Alright so I’ve been on the ETH train for some...   \n",
       "2     15  i tried almost a year and a half ago to get in...   \n",
       "3     16  Hi guys, I am not very familiar with ethereum ...   \n",
       "4     21  Hi everyone, I think this is the most suited s...   \n",
       "5     23  Spoiler alert.. a lot.  \\nWe go frontrun on tr...   \n",
       "6     24  So The Secret Network Ethereum bridge is live....   \n",
       "7     29  Hi ethereum holders/investors/stakers etc.\\n\\n...   \n",
       "8     33  \\nIf you’ve been keeping up with Ripple then y...   \n",
       "9     35  I know the basics of crypto but I’m not the mo...   \n",
       "\n",
       "                                               title  subreddit  \\\n",
       "0       16 Ethereum Predictions From a Crypto Oracle          1   \n",
       "1                                   Upside of Ether?          1   \n",
       "2                                        i need help          1   \n",
       "3  New to this... is now a good time to invest in...          1   \n",
       "4                                    Should I stake?          1   \n",
       "5  We took a tour around the dark forest to see h...          1   \n",
       "6  More secretEther users needed! Privacy fans un...          1   \n",
       "7                                  Max supply of ETH          1   \n",
       "8                                  XRP right now lol          1   \n",
       "9                                  Is this a bubble?          1   \n",
       "\n",
       "                                                text  status_length  \\\n",
       "0  \\nhttpswwwcoindeskcom16ethereumpredictionscryp...             97   \n",
       "1  alright so ive been on the eth train for some ...            705   \n",
       "2  i tried almost a year and a half ago to get in...            816   \n",
       "3  hi guys i am not very familiar with ethereum b...            390   \n",
       "4  hi everyone i think this is the most suited su...            662   \n",
       "5  spoiler alert a lot  \\nwe go frontrun on trans...            396   \n",
       "6  so the secret network ethereum bridge is live ...            874   \n",
       "7  hi ethereum holdersinvestorsstakers etc\\n\\nis ...            142   \n",
       "8  \\nif youve been keeping up with ripple then yo...            272   \n",
       "9  i know the basics of crypto but im not the mos...            539   \n",
       "\n",
       "   word_count  sentiment  \n",
       "0           7     0.0000  \n",
       "1         142     0.9724  \n",
       "2         156     0.9355  \n",
       "3          76     0.9109  \n",
       "4         129     0.9294  \n",
       "5          46     0.7269  \n",
       "6         124     0.9801  \n",
       "7          27     0.1280  \n",
       "8          42     0.8909  \n",
       "9         105     0.7301  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.to_csv('./datasets/posts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    " - what is our baseline model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.531871\n",
       "0    0.468129\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 53% of the time, we can expect a subreddit from the data to be from the Bitcoin subreddit. This is our baseline case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models \n",
    "\n",
    "### CountVectorizer/TfidVectorizer in RandomForest/AdaBoost/Bagging Classifiers\n",
    "\n",
    "- In this section, I'm going to model my text using CountVectorizer/TfidVectorizer transformers in RandomForest/AdaBoost/BaggingClassifier models. \n",
    "- Within the CountVectorizer, I'm using a LemmaTokenizer to simplify some of the text data.  \n",
    "- For the purpose of my problem state, I'm going to use a feature union within my pipelines and run a gridsearch on all models. \n",
    "- By the end I will pick the best model and fine tune the hyperparameters for optimal performance. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9805 entries, 0 to 10631\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   index          9805 non-null   int64  \n",
      " 1   selftext       9805 non-null   object \n",
      " 2   title          9805 non-null   object \n",
      " 3   subreddit      9805 non-null   int64  \n",
      " 4   text           9805 non-null   object \n",
      " 5   status_length  9805 non-null   int64  \n",
      " 6   word_count     9805 non-null   int64  \n",
      " 7   sentiment      9805 non-null   float64\n",
      "dtypes: float64(1), int64(4), object(3)\n",
      "memory usage: 689.4+ KB\n"
     ]
    }
   ],
   "source": [
    "posts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.524426\n",
       "1    0.475574\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 1053,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True) # baseline model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with sentiment \n",
    "\n",
    "get_text_data = FunctionTransformer(lambda x: x['text'], validate = False)\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[['sentiment']], validate = False)\n",
    "\n",
    "X = posts[['text', 'sentiment']]\n",
    "y = posts['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)\n",
    "\n",
    "cvec_pipe1 = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('numeric_features', Pipeline([\n",
    "           ('selector', get_numeric_data), \n",
    "            ('ss', StandardScaler())\n",
    "        ])),\n",
    "        ('text_features', Pipeline([\n",
    "        ('selector', get_text_data), \n",
    "        ('cvec', CountVectorizer(tokenizer=LemmaTokenizer(), stop_words=my_stop_words))\n",
    "        ])),  \n",
    "    ])),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "cvec_pipe2 = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('numeric_features', Pipeline([\n",
    "           ('selector', get_numeric_data), \n",
    "            ('ss', StandardScaler())\n",
    "        ])),\n",
    "        ('text_features', Pipeline([\n",
    "        ('selector', get_text_data), \n",
    "        ('cvec', CountVectorizer(tokenizer=LemmaTokenizer(), stop_words=my_stop_words))\n",
    "        ])), \n",
    "    ])),\n",
    "    ('ada', AdaBoostClassifier())\n",
    "])\n",
    "\n",
    "cvec_pipe3 = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('numeric_features', Pipeline([\n",
    "           ('selector', get_numeric_data), \n",
    "            ('ss', StandardScaler())\n",
    "        ])),\n",
    "        ('text_features', Pipeline([\n",
    "        ('selector', get_text_data), \n",
    "        ('cvec', CountVectorizer(tokenizer=LemmaTokenizer(), stop_words=my_stop_words))\n",
    "        ])), \n",
    "    ])),\n",
    "    ('bag', BaggingClassifier())\n",
    "])\n",
    "\n",
    "tvec_pipe1 = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('numeric_features', Pipeline([\n",
    "           ('selector', get_numeric_data), \n",
    "            ('ss', StandardScaler())\n",
    "        ])),\n",
    "        ('text_features', Pipeline([\n",
    "        ('selector', get_text_data), \n",
    "        ('tvec', TfidfVectorizer(tokenizer=LemmaTokenizer(), stop_words=my_stop_words))\n",
    "        ])), \n",
    "    ])),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "tvec_pipe2 = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('numeric_features', Pipeline([\n",
    "           ('selector', get_numeric_data), \n",
    "            ('ss', StandardScaler())\n",
    "        ])),\n",
    "        ('text_features', Pipeline([\n",
    "        ('selector', get_text_data), \n",
    "        ('tvec', TfidfVectorizer(tokenizer=LemmaTokenizer(), stop_words=my_stop_words))\n",
    "        ])), \n",
    "    ])),\n",
    "    ('ada', AdaBoostClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "tvec_pipe3 = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('numeric_features', Pipeline([\n",
    "           ('selector', get_numeric_data), \n",
    "            ('ss', StandardScaler())\n",
    "        ])),\n",
    "        ('text_features', Pipeline([\n",
    "        ('selector', get_text_data), \n",
    "        ('tvec', TfidfVectorizer(tokenizer=LemmaTokenizer(), stop_words=my_stop_words))\n",
    "        ])), \n",
    "    ])),\n",
    "    ('bag', BaggingClassifier())\n",
    "])\n",
    "\n",
    "cvec_pipe_params = {\n",
    "    'features__text_features__cvec__max_features': [5_000, 6000, 7000],\n",
    "    'features__text_features__cvec__min_df': [2, 3],\n",
    "    'features__text_features__cvec__max_df': [.5, .7, .9],\n",
    "    'features__text_features__cvec__ngram_range': [(1,1)]\n",
    "}\n",
    "tvec_pipe_params = {\n",
    "    'features__text_features__tvec__max_features': [5_000, 6000, 7000],\n",
    "    'features__text_features__tvec__min_df': [2, 3],\n",
    "    'features__text_features__tvec__max_df': [.5, .7, .9],\n",
    "    'features__text_features__tvec__ngram_range': [(1,1)]\n",
    "}\n",
    "\n",
    "cvec_pipes = [cvec_pipe1, cvec_pipe2, cvec_pipe3]\n",
    "cvec_grids = []\n",
    "for pipe in cvec_pipes: \n",
    "    gs = GridSearchCV(pipe, \n",
    "                      param_grid=cvec_pipe_params,\n",
    "                      cv=5, \n",
    "                      verbose=2,\n",
    "                      ) # 5-fold cross-validation.\n",
    "    cvec_grids.append(gs)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "tvec_pipes = [tvec_pipe1, tvec_pipe2, tvec_pipe3]\n",
    "tvec_grids = []\n",
    "for pipe in tvec_pipes: \n",
    "    gs = GridSearchCV(pipe,\n",
    "                      param_grid=tvec_pipe_params, \n",
    "                      cv=5, \n",
    "                      verbose=2,\n",
    "                      ) # 5-fold cross-validation.\n",
    "    tvec_grids.append(gs)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "best_cvec_scores = []\n",
    "best_cvec_train_scores = []\n",
    "best_cvec_test_scores = []\n",
    "best_cvec_params = []\n",
    "\n",
    "for grid in cvec_grids: \n",
    "\n",
    "    #print(grid)\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(f'Best Score: {grid.best_score_} Train Set Score: {grid.score(X_train, y_train)} Test Set Score: {grid.score(X_test, y_test)} Best Parameters: {grid.best_params_}')\n",
    "    best_cvec_scores.append(grid.best_score_)\n",
    "    best_cvec_train_scores.append(grid.score(X_train, y_train))\n",
    "    best_cvec_test_scores.append(grid.score(X_test, y_test))\n",
    "    best_cvec_params.append(grid.best_params_)\n",
    "\n",
    "best_tvec_scores = []\n",
    "best_tvec_train_scores = []\n",
    "best_tvec_test_scores = []\n",
    "best_tvec_params = []   \n",
    "\n",
    "for grid in tvec_grids: \n",
    "\n",
    "    #print(grid)\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(f'Best Score: {grid.best_score_} Train Set Score: {grid.score(X_train, y_train)} Test Set Score {grid.score(X_test, y_test)} Best Parameters: {grid.best_params_}')\n",
    "    \n",
    "    best_tvec_scores.append(grid.best_score_)\n",
    "    best_tvec_train_scores.append(grid.score(X_train, y_train))\n",
    "    best_tvec_test_scores.append(grid.score(X_test, y_test))\n",
    "    best_tvec_params.append(grid.best_params_)\n",
    "    \n",
    "    print(f' This cell took {time.time() - t0} seconds to run')\n",
    "\n",
    "\n",
    "print(f'Best cvec scores: {best_cvec_scores}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best tvec scores : [0.8392476128540146, 0.7801827179556733, 0.7970786225092479]\n"
     ]
    }
   ],
   "source": [
    "print(f'Best tvec scores : {best_tvec_scores}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cvec train scores: [0.9986299284518192, 0.7954026488049931, 0.9879738164104126]\n",
      "Best cvec test scores : [0.8389987639060569, 0.792027194066749, 0.8062422744128553]\n"
     ]
    }
   ],
   "source": [
    "print(f'Best cvec train scores: {best_cvec_train_scores}')\n",
    "print(f'Best cvec test scores : {best_cvec_test_scores}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best tvec train scores: [0.9986299284518192, 0.7970771806972142, 0.9891916577865733]\n",
      "Best tvec test scores: [0.8411619283065513, 0.7877008652657602, 0.804079110012361]\n"
     ]
    }
   ],
   "source": [
    "print(f'Best tvec train scores: {best_tvec_train_scores}')\n",
    "print(f'Best tvec test scores: {best_tvec_test_scores}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.7s remaining:    0.0s\n",
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 1), total=   4.7s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 1), total=   4.7s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 2), total=   5.7s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 2), total=   5.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 2), total=   5.7s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 2), total=   5.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 2), total=   5.6s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 2), total=   5.7s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 2), total=   5.7s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 2), total=   5.7s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 2), total=   5.7s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=2, tvec__ngram_range=(1, 2), total=   5.7s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 1), total=   5.0s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 2), total=   5.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 2), total=   5.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 2), total=   5.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 2), total=   5.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 2), total=   6.1s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 2), total=   6.1s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 2), total=   5.7s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 2), total=   5.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 2), total=   5.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.01, tvec__ngram_range=(1, 2), total=   5.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 2), total=   5.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 2), total=   6.0s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 2), total=   6.0s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 2), total=   5.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 2), total=   5.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 2), total=   5.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 2), total=   5.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 2), total=   5.7s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 2), total=   5.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=6000, tvec__min_df=0.02, tvec__ngram_range=(1, 2), total=   5.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 1), total=   5.0s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 1), total=   4.7s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 2), total=   5.7s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 2), total=   5.6s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 2), total=   5.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 2), total=   5.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 2), total=   5.7s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 2), total=   5.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 2), total=   5.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 2), total=   5.7s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 2), total=   5.7s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=2, tvec__ngram_range=(1, 2), total=   5.7s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 1), total=   4.7s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 2), total=   5.7s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 2), total=   5.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 2), total=   6.0s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 2), total=   5.7s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 2), total=   5.7s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 2), total=   6.0s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 2), total=   5.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 2), total=   6.0s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 2), total=   5.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.01, tvec__ngram_range=(1, 2), total=   5.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 1), total=   5.0s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 1), total=   5.2s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 1), total=   5.1s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 2), total=   6.0s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 2), total=   5.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 2), total=   6.0s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 2), total=   6.1s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 2), total=   6.1s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 2), total=   6.0s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 2), total=   5.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 2), total=   5.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 2), total=   5.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=7000, tvec__min_df=0.02, tvec__ngram_range=(1, 2), total=   6.2s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 1), total=   5.0s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 1), total=   5.3s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 1), total=   5.2s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 1), total=   5.0s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 1), total=   5.0s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 1), total=   5.1s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 1), total=   5.0s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 1), total=   5.0s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 1), total=   5.0s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 2), total=   6.3s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 2), total=   6.2s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 2), total=   6.2s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 2), total=   6.1s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 2), total=   5.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 2), total=   5.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 2), total=   6.0s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 2), total=   5.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 2), total=   5.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=2, tvec__ngram_range=(1, 2), total=   5.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 1), total=   5.0s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 1), total=   5.0s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 1), total=   5.0s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 1), total=   5.0s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 1), total=   5.0s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 2), total=   5.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 2), total=   5.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 2), total=   5.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 2), total=   6.0s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 2), total=   5.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 2), total=   5.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 2), total=   5.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 2), total=   5.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 2), total=   6.0s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.01, tvec__ngram_range=(1, 2), total=   5.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 1), total=   5.0s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 1), total=   4.7s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 1), total=   5.0s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 1), total=   5.1s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 2), total=   5.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 2), total=   5.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 2), total=   5.8s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 2), total=   5.7s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 2), total=   5.7s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 2), total=   5.7s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 2), total=   5.6s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 2), total=   5.6s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 2), total=   5.7s\n",
      "[CV] tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tvec__max_df=0.2, tvec__max_features=8000, tvec__min_df=0.02, tvec__ngram_range=(1, 2), total=   5.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 180 out of 180 | elapsed: 16.1min finished\n",
      "C:\\Users\\tsuts\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.8677193201439597 Train Set Score: 0.9036383011112803 Test Set Score: 0.9032756489493201 Best Parameters: {'tvec__max_df': 0.2, 'tvec__max_features': 7000, 'tvec__min_df': 2, 'tvec__ngram_range': (1, 1)}\n",
      " This cell took 978.1077628135681 seconds to run\n"
     ]
    }
   ],
   "source": [
    "# without sentiment \n",
    "X = posts['text']\n",
    "y = posts['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('tvec', TfidfVectorizer(tokenizer=LemmaTokenizer(), stop_words=my_stop_words)), \n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'tvec__max_features': [6000,7000, 8000],\n",
    "    'tvec__min_df': [2, .01, .02],\n",
    "    'tvec__max_df': [.2],\n",
    "    'tvec__ngram_range': [(1,1), (1,2)]\n",
    "}\n",
    "\n",
    "\n",
    "gs = GridSearchCV(pipe, # what object are we optimizing?\n",
    "                      param_grid=pipe_params, # what parameters values are we searching?\n",
    "                      cv=10, \n",
    "                      verbose=2,\n",
    "                      ) # 5-fold cross-validation.\n",
    "t0 = time.time()\n",
    "\n",
    "gs.fit(X, y)\n",
    "print(f'Best Score: {gs.best_score_} Train Set Score: {gs.score(X_train, y_train)} Test Set Score: {gs.score(X_test, y_test)} Best Parameters: {gs.best_params_}')\n",
    "\n",
    "print(f' This cell took {time.time() - t0} seconds to run')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The best model from the GridSearch is the RandomForest model using a CountVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the next notebook, I'm going to tune my hyperparamaters on my best models and determine my best model. \n",
    "- From there, I will interpret the results and see what we can learn from r/Bitcoin and r/Ethereum. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
